{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm-fc.ipynb","provenance":[],"collapsed_sections":["DqoFJzXH0dqz"],"mount_file_id":"1MXLfXnemf9MU1Y0Oaar6g5XazyJcH6Zo","authorship_tag":"ABX9TyNxYuR5X1175pMaXTk+4CkZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Train the LSTM-FC baseline with spatial correction\n","\n","![picture](https://repository.ust.hk/ir/profileImages/xclu.jpg)![picture](https://envrpg.ust.hk/stuphotos/xluad.jpg)\n","\n","This notebook is for building the LSTM-FC models with the spatial correction. For Google Colab only."],"metadata":{"id":"RKNf64tez7om"}},{"cell_type":"markdown","source":["## Set up the environment\n","\n","### Import the packages"],"metadata":{"id":"DqoFJzXH0dqz"}},{"cell_type":"code","source":["! pip install geopy\n","! pip install pykrige\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import os, sys\n","import numpy as np"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BR0WKk5S0dJM","executionInfo":{"status":"ok","timestamp":1648122445706,"user_tz":-480,"elapsed":8539,"user":{"displayName":"Jeff Haochen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTzv2dttIh9GpgB-OyZFyFUZWStut-j1kN83pA=s64","userId":"12138346426282739410"}},"outputId":"826def91-4285-4fd4-d7ea-cda8f4326475"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: geopy in /usr/local/lib/python3.7/dist-packages (1.17.0)\n","Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy) (1.52)\n","Requirement already satisfied: pykrige in /usr/local/lib/python3.7/dist-packages (1.6.1)\n","Requirement already satisfied: numpy<2,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from pykrige) (1.21.5)\n","Requirement already satisfied: scipy<2,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pykrige) (1.4.1)\n"]}]},{"cell_type":"markdown","source":["### Training device"],"metadata":{"id":"-7Ps8Fl05v-b"}},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(f'Using device {device}...')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gHBU76Ze5yn_","executionInfo":{"status":"ok","timestamp":1648122446886,"user_tz":-480,"elapsed":10,"user":{"displayName":"Jeff Haochen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTzv2dttIh9GpgB-OyZFyFUZWStut-j1kN83pA=s64","userId":"12138346426282739410"}},"outputId":"ec6e75f9-c601-4d01-d34f-7cf1a070c6ef"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device cuda...\n"]}]},{"cell_type":"markdown","source":["### Saving and loading files to/from different places\n","\n","- **Temporary files** will be saved in the COLAB workspace directory\n","- **Models** and other files necessary for long-term use will be saved in the drive directory "],"metadata":{"id":"Tkfr5fsa1XTK"}},{"cell_type":"code","source":["workspace_dir = '/content'\n","drive_dir = '/content/drive/Othercomputers/DESKTOP-P14JC7J/2130/code'\n","sys.path.append(drive_dir)\n","\n","from data_utils import *"],"metadata":{"id":"XoKw1uHdz9uf","executionInfo":{"status":"ok","timestamp":1648122449468,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jeff Haochen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTzv2dttIh9GpgB-OyZFyFUZWStut-j1kN83pA=s64","userId":"12138346426282739410"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Copy the processed data here\n","\n","The processed data should be saved on the Google drive."],"metadata":{"id":"zXDOW8VQCOqc"}},{"cell_type":"code","source":["! mkdir data\n","! mkdir baseline-results\n","! cp ./drive/MyDrive/urop-paper2-data/* ./data\n","! cp ./drive/MyDrive/urop-paper2-baseline-results/* ./baseline-results"],"metadata":{"id":"xeAor_H2CWjr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648122455809,"user_tz":-480,"elapsed":4106,"user":{"displayName":"Jeff Haochen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTzv2dttIh9GpgB-OyZFyFUZWStut-j1kN83pA=s64","userId":"12138346426282739410"}},"outputId":"fbbce79b-5afb-494d-bb06-342957e953ec"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘data’: File exists\n","mkdir: cannot create directory ‘baseline-results’: File exists\n"]}]},{"cell_type":"markdown","source":["## Dataset\n","The dataset used for training and testing."],"metadata":{"id":"8Ju2PT090r8U"}},{"cell_type":"code","source":["from data_utils.obs import ObsReader\n","\n","from torch.utils.data import Dataset\n","\n","from datetime import datetime, date, time, timedelta\n","\n","class BaselineSourceDataset(Dataset):\n","  def __init__(self, train, device):\n","    super().__init__()\n","    self.period = 'train' if train else 'test'\n","    self.device = device\n","    self.first_date = eval(f'{self.period}_first_dt').date()\n","    self.last_date = eval(f'{self.period}_last_dt').date()\n","\n","    self.obs_normalizing = load_dict(f'{data_dir}/obs_normalizing.pkl')\n","    self.wrf_normalizing = np.load(f'{data_dir}/wrf_normalizing.npz')\n","    self.wrf_normalizing = (self.wrf_normalizing['mean'], self.wrf_normalizing['std'])\n","    self.cmaq_normalizing = np.load(f'{data_dir}/cmaq_normalizing.npz')\n","    self.cmaq_normalizing = (self.cmaq_normalizing['mean'], self.cmaq_normalizing['std'])\n","    \n","    source_obs_data = load_dict(f'{data_dir}/{self.period}_source_data.pkl')\n","    source_wrf_match = load_dict(f'{data_dir}/source_wrf_match.pkl')\n","    source_cmaq_match = load_dict(f'{data_dir}/source_cmaq_match.pkl')\n","    assert list(source_obs_data.keys()) == list(source_wrf_match.keys()) == list(source_cmaq_match.keys())\n","    self.source_stations = list(source_obs_data.keys())\n","\n","    target_obs_data = load_dict(f'{data_dir}/{self.period}_target_data.pkl')\n","\n","    self.source_obs_reader = ObsReader(source_obs_data)\n","    self.target_obs_reader = ObsReader(target_obs_data, self.source_stations)\n","\n","    target_wrf_data = np.load(f'{data_dir}/{self.period}_source_wrf_data.npy')\n","    target_cmaq_data = np.load(f'{data_dir}/{self.period}_source_cmaq_data.npy')\n","        \n","    self.target_wrf_data, self.target_cmaq_data = target_wrf_data, target_cmaq_data\n","\n","    assert len(self) == self.target_wrf_data.shape[0] == self.target_cmaq_data.shape[0]\n","\n","  def get_source_obs(self, day0):\n","    first_dt = datetime.combine(day0-timedelta(days = history_days), time(0))\n","    last_dt = datetime.combine(day0-timedelta(days = 1), time(23))\n","    source_obs = self.source_obs_reader(first_dt, last_dt)\n","    out = {}\n","    for st, df in source_obs.items():\n","      means, stds = np.array([self.obs_normalizing[sp][0] for sp in df.columns]), np.array([self.obs_normalizing[sp][1] for sp in df.columns])\n","      out[st] = torch.nan_to_num(torch.tensor(((df.values - means) / stds).T, dtype = torch.float, device = self.device))\n","    return out\n","\n","  def get_target_obs(self, day0):\n","    means, stds = np.array([self.obs_normalizing[sp][0] for sp in target_species]), np.array([self.obs_normalizing[sp][1] for sp in target_species])\n","    first_dt = datetime.combine(day0, time(0))\n","    last_dt = datetime.combine(day0 + timedelta(days = horizon_days - 1), time(23))\n","    target_obs = self.target_obs_reader(first_dt, last_dt)\n","    return torch.tensor(np.array([((df.values - means)/stds).T for df in target_obs.values()]), dtype = torch.float, device = self.device)\n","\n","  def __getitem__(self, index):\n","    day0 = self.first_date + timedelta(days = history_days + index)\n","\n","    source_obs = self.get_source_obs(day0)\n","    target_wrf = torch.tensor((self.target_wrf_data[index] - self.wrf_normalizing[0][:, None])/self.wrf_normalizing[1][:, None], dtype = torch.float, device = self.device)\n","    target_cmaq = torch.tensor((self.target_cmaq_data[index] - self.cmaq_normalizing[0][:, None])/self.cmaq_normalizing[1][:, None], dtype = torch.float, device = self.device)\n","    target_wrf_cmaq = torch.cat([target_wrf, target_cmaq], dim = -2)\n","\n","    target_obs = self.get_target_obs(day0)\n","\n","    return source_obs, target_wrf_cmaq, target_obs\n","\n","  def __len__(self):\n","    return (self.last_date - self.first_date).days - (history_days + horizon_days) + 2\n","\n","def regional_dataset_collate_fn(batch):\n","  source_obs_out, target_wrf_cmaq_out, target_obs_out = [], [], []\n","  for source_obs, target_wrf_cmaq, target_obs in batch:\n","    source_obs_out.append(source_obs)\n","    target_wrf_cmaq_out.append(target_wrf_cmaq)\n","    target_obs_out.append(target_obs)\n","  return torch.utils.data._utils.collate.default_collate(source_obs_out), torch.stack(target_wrf_cmaq_out, axis = 1), torch.stack(target_obs_out, axis = 1)"],"metadata":{"id":"exvQLYND0vo7","executionInfo":{"status":"ok","timestamp":1648122475224,"user_tz":-480,"elapsed":318,"user":{"displayName":"Jeff Haochen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTzv2dttIh9GpgB-OyZFyFUZWStut-j1kN83pA=s64","userId":"12138346426282739410"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Training\n","### Hyperparameters\n","Hyperparameters for training the model and model configurations."],"metadata":{"id":"iLnkGhh527xW"}},{"cell_type":"code","source":["num_epoch = 32\n","batch_size = 128\n","learning_rate, gamma, step_size = 1e-3, 0.9, 100"],"metadata":{"id":"v6gfx9dS29b3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Initialization\n","Build the models, optimizers and learning rate schedulers, and the dataset"],"metadata":{"id":"DRIcg2v33dSb"}},{"cell_type":"code","source":["# the model, the optimizer and the scheduler\n","from baselines.lstm_fc.model import LSTM_FC\n","models = nn.ModuleDict({st: LSTM_FC(len(df.columns)) for st, df in load_dict(f'{data_dir}/train_source_data.pkl').items()}).to(device)\n","optimizer = torch.optim.Adam(models.parameters(), lr = learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma = gamma, step_size = step_size)"],"metadata":{"id":"oq7sEuTx3jLf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# the training set and the distance used\n","training_set = BaselineSourceDataset(train = True, device = device)\n","training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, collate_fn=regional_dataset_collate_fn)\n","\n","# the validation set and the distance used\n","validation_set = BaselineSourceDataset(train = False, device = device)\n","validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=True, collate_fn=regional_dataset_collate_fn)"],"metadata":{"id":"6-eJ2jcz3rWq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training loop"],"metadata":{"id":"BGQsodde6KP1"}},{"cell_type":"code","source":["for i in range(num_epoch):\n","  models.train()\n","  train_epoch_loss = 0.0\n","  for source_obs, target_wrf_cmaq, target_obs in training_loader:\n","    optimizer.zero_grad()\n","    out = [models[st](X0, X1) for (st, X0), X1 in zip(source_obs.items(), target_wrf_cmaq)]\n","    out = torch.stack(out)\n","    \n","    mask = ~torch.isnan(target_obs)\n","    loss = torch.abs(out[mask] - target_obs[mask]).mean()\n","    with torch.no_grad():\n","      train_epoch_loss += loss.item() * target_obs.shape[1] / len(training_set)\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","\n","  valid_epoch_loss = 0.0\n","  models.eval()\n","  with torch.no_grad():\n","    for source_obs, target_wrf_cmaq, target_obs in validation_loader:\n","      out = [models[st](X0, X1) for (st, X0), X1 in zip(source_obs.items(), target_wrf_cmaq)]\n","      out = torch.stack(out)\n","      \n","      mask = ~torch.isnan(target_obs)\n","      loss = torch.abs(out[mask] - target_obs[mask]).mean()\n","      valid_epoch_loss += loss.item() * target_obs.shape[1] / len(validation_set)\n","\n","  print(f'Epoch {i}, training loss {train_epoch_loss:.3g}, validation loss {valid_epoch_loss:.3g}...')\n","  if train_epoch_loss < valid_epoch_loss:\n","    break"],"metadata":{"id":"iWFSwPEQ6L3T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647955506345,"user_tz":-480,"elapsed":1354208,"user":{"displayName":"Jeff Haochen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTzv2dttIh9GpgB-OyZFyFUZWStut-j1kN83pA=s64","userId":"12138346426282739410"}},"outputId":"b1376430-523b-4320-801a-3b8783843771"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, training loss 0.645, validation loss 0.537...\n","Epoch 1, training loss 0.578, validation loss 0.466...\n","Epoch 2, training loss 0.532, validation loss 0.427...\n","Epoch 3, training loss 0.5, validation loss 0.396...\n","Epoch 4, training loss 0.477, validation loss 0.38...\n","Epoch 5, training loss 0.459, validation loss 0.376...\n","Epoch 6, training loss 0.447, validation loss 0.367...\n","Epoch 7, training loss 0.438, validation loss 0.36...\n","Epoch 8, training loss 0.431, validation loss 0.347...\n","Epoch 9, training loss 0.426, validation loss 0.357...\n","Epoch 10, training loss 0.42, validation loss 0.344...\n","Epoch 11, training loss 0.417, validation loss 0.344...\n","Epoch 12, training loss 0.412, validation loss 0.345...\n","Epoch 13, training loss 0.408, validation loss 0.348...\n","Epoch 14, training loss 0.405, validation loss 0.332...\n","Epoch 15, training loss 0.403, validation loss 0.337...\n","Epoch 16, training loss 0.4, validation loss 0.341...\n","Epoch 17, training loss 0.398, validation loss 0.34...\n","Epoch 18, training loss 0.396, validation loss 0.337...\n","Epoch 19, training loss 0.394, validation loss 0.341...\n","Epoch 20, training loss 0.392, validation loss 0.338...\n","Epoch 21, training loss 0.391, validation loss 0.334...\n","Epoch 22, training loss 0.388, validation loss 0.331...\n","Epoch 23, training loss 0.388, validation loss 0.334...\n","Epoch 24, training loss 0.385, validation loss 0.336...\n","Epoch 25, training loss 0.384, validation loss 0.335...\n","Epoch 26, training loss 0.383, validation loss 0.326...\n","Epoch 27, training loss 0.381, validation loss 0.325...\n","Epoch 28, training loss 0.38, validation loss 0.324...\n","Epoch 29, training loss 0.378, validation loss 0.338...\n","Epoch 30, training loss 0.377, validation loss 0.325...\n","Epoch 31, training loss 0.376, validation loss 0.333...\n"]}]},{"cell_type":"markdown","source":["### Save the trained model"],"metadata":{"id":"eIs_CEPz6ee5"}},{"cell_type":"code","source":["import json\n","model_dir = f'{drive_dir}/models/baseline_lstm_fc'\n","os.makedirs(model_dir, exist_ok = True)\n","torch.save(models.state_dict(), f'{model_dir}/state_dict')\n","torch.save(optimizer.state_dict(), f'{model_dir}/optimizer_state_dict')\n","torch.save(scheduler.state_dict(), f'{model_dir}/scheduler_state_dict')\n","with open (f'{model_dir}/training_config.json', 'w') as f:\n","  json.dump({\n","    'num_epoch': num_epoch, \n","    'batch_size': batch_size,\n","    'learning_rate': learning_rate,\n","    'gamma': gamma,\n","    'step_size': step_size\n","  }, f)"],"metadata":{"id":"rkHH3jZC6fvq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prediction\n","Test on the testing target stations that are neither training target stations nor source stations, report the results on multiple metrics.\n","\n","\n","### Load the trained model\n"],"metadata":{"id":"ABCe0la9dE3M"}},{"cell_type":"code","source":["from model import Regional\n","model_dir = f'{drive_dir}/models/baseline_lstm_fc'\n","models = nn.ModuleDict({st: LSTM_FC(len(df.columns)) for st, df in load_dict(f'{data_dir}/train_source_data.pkl').items()}).to(device)\n","models.load_state_dict(torch.load(f'{model_dir}/state_dict'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"p8rWbOj3YOxM","executionInfo":{"status":"error","timestamp":1648122313443,"user_tz":-480,"elapsed":1122,"user":{"displayName":"Jeff Haochen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTzv2dttIh9GpgB-OyZFyFUZWStut-j1kN83pA=s64","userId":"12138346426282739410"}},"outputId":"d162f42c-3435-4799-fbc0-a185ab405612"},"execution_count":8,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-ab336696b54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{drive_dir}/models/baseline_lstm_fc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLSTM_FC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{data_dir}/train_source_data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{model_dir}/state_dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/Othercomputers/DESKTOP-P14JC7J/2130/code/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeq2seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectionalLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimestepDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcasting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model_utils.utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["### Load the testing data"],"metadata":{"id":"Fth4SaCfaGhr"}},{"cell_type":"code","source":["# Only the testing target stations that are neither training target stations nor source stations\n","source_loc = load_dict(f'{data_dir}/source_loc.pkl')\n","source_stations = set(source_loc.keys())\n","\n","train_target_stations = set(load_dict(f'{data_dir}/train_target_loc.pkl').keys())\n","test_target_loc = load_dict(f'{data_dir}/test_target_loc.pkl')\n","test_target_stations = set(test_target_loc.keys())\n","\n","test_target_stations = list(test_target_stations - train_target_stations - source_stations)\n","test_target_loc = {st: test_target_loc[st] for st in test_target_stations}"],"metadata":{"id":"AEv1JvwAwK32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# denormalize the output\n","obs_normalizing = load_dict(f'{data_dir}/obs_normalizing.pkl')\n","means, stds = np.array([obs_normalizing[sp][0] for sp in target_species]), np.array([obs_normalizing[sp][1] for sp in target_species])"],"metadata":{"id":"ifugX-2uu5ty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the testing set\n","testing_set = BaselineSourceDataset(train = False, device = device)\n","testing_loader = torch.utils.data.DataLoader(testing_set, batch_size=batch_size, collate_fn=regional_dataset_collate_fn)"],"metadata":{"id":"qeqrFv24dGzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models.eval()\n","assert not models.training"],"metadata":{"id":"ouibFX-xvTqq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = {st: [] for st in testing_set.source_stations}\n","with torch.no_grad():\n","  testing_loss = 0.0\n","  for source_obs, target_wrf_cmaq, target_obs in testing_loader:\n","    assert list(source_obs.keys()) == list(models.keys())\n","    for (st, model), (st, X0), X1 in zip(models.items(), source_obs.items(), target_wrf_cmaq):\n","      pred[st].append(model.predict(X0, X1, (means, stds)))\n","\n","for st in testing_set.source_stations:\n","  pred[st] = np.concatenate(pred[st])\n","  print(st, pred[st].shape)"],"metadata":{"id":"jxoDGtd5dWEN","executionInfo":{"status":"ok","timestamp":1647958245026,"user_tz":-480,"elapsed":8130,"user":{"displayName":"Jeff Haochen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTzv2dttIh9GpgB-OyZFyFUZWStut-j1kN83pA=s64","userId":"12138346426282739410"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b4be310-560d-4a08-a8c5-7163a6a27791"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CN_1379A (361, 2, 48)\n","SP_A (361, 2, 48)\n","XCNAQ437 (361, 2, 48)\n","CN_1394A (361, 2, 48)\n","CN_1370A (361, 2, 48)\n","TW_A (361, 2, 48)\n","CW_A (361, 2, 48)\n","CN_1365A (361, 2, 48)\n","TP_A (361, 2, 48)\n","CN_1369A (361, 2, 48)\n","XCNAQ1145 (361, 2, 48)\n","CL_R (361, 2, 48)\n","YL_A (361, 2, 48)\n","EN_A (361, 2, 48)\n","CN_1392A (361, 2, 48)\n","TC_A (361, 2, 48)\n","CN_1396A (361, 2, 48)\n","XCNAQ418 (361, 2, 48)\n","XCNAQ1144 (361, 2, 48)\n","CN_1391A (361, 2, 48)\n","XCNAQ445 (361, 2, 48)\n","CN_1364A (361, 2, 48)\n","MKaR (361, 2, 48)\n","XCNAQ439 (361, 2, 48)\n","CN_1380A (361, 2, 48)\n","CN_1381A (361, 2, 48)\n","CN_1357A (361, 2, 48)\n","KC_A (361, 2, 48)\n","TM_A (361, 2, 48)\n","KT_A (361, 2, 48)\n","CN_1358A (361, 2, 48)\n","CB_R (361, 2, 48)\n"]}]},{"cell_type":"markdown","source":["### Interpolation"],"metadata":{"id":"V45ljf7b0hNZ"}},{"cell_type":"code","source":["from data_utils.interpolation import *\n","source_cmaq_pred = load_dict('data/test_source_cmaq')"],"metadata":{"id":"5khiwy4e2Sl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cmaq_pred = load_dict('baseline-results/cmaq_pred.pkl')\n","for st in test_target_stations:\n","  print(st, cmaq_pred[st].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xuWsvTHB2eEk","executionInfo":{"status":"ok","timestamp":1647958080399,"user_tz":-480,"elapsed":598,"user":{"displayName":"Jeff Haochen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTzv2dttIh9GpgB-OyZFyFUZWStut-j1kN83pA=s64","userId":"12138346426282739410"}},"outputId":"2c5a005a-0b0d-455b-b957-ef20806fcfab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["XCNAQ3249 (361, 2, 48)\n","XCNAQ3278 (361, 2, 48)\n","TK_A (361, 2, 48)\n","XCNAQ3238 (361, 2, 48)\n","XCNAQ3290 (361, 2, 48)\n","XCNAQ1295 (361, 2, 48)\n","XCNAQ3237 (361, 2, 48)\n","XCNAQ3282 (361, 2, 48)\n","NH_A (361, 2, 48)\n","XCNAQ1296 (361, 2, 48)\n","MACG (361, 2, 48)\n","XCNAQ2914 (361, 2, 48)\n","XCNAQ3260 (361, 2, 48)\n","SN_A (361, 2, 48)\n","XCNAQ3279 (361, 2, 48)\n","XCNAQ3950 (361, 2, 48)\n","MACT (361, 2, 48)\n","XCNAQ3277 (361, 2, 48)\n","MCKO (361, 2, 48)\n","MACH (361, 2, 48)\n","MACC (361, 2, 48)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"NFouTINZ8U-L"},"execution_count":null,"outputs":[]}]}